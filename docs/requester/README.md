# Requester Documentation

This is the documentation for data requesters who want to use Crowdaq to improve their data collection workflows.

## Overview
As a highlight, what you can do typically on Crowdaq's website are:
- Build annotation interfaces declaratively without knowing HTML/CSS/JavaScript
- Design qualification exams declaratively without knowing how to set up a backend server
- Monitor how well people are doing on your qualification exams
- Monitor the annotation progress and inter-annotator agreement of your main task sets

What you can do typically with Crowdaq's client app is:
- Easily download necessary files for sharing/reproducibility purposes
- Launch your exams or task sets onto Amazon Mechanical Turk (MTurk)
- Manage your HITs on MTurk


## Table of Content
1. [Get Started](get-started.md): We will walk through the main features in Crowdaq.
2. [UI Design](ui.md): How to design annotation user-interface (UI).
3. [Launch](launch.md): How to launch your exams and task sets to MTurk.
4. [Release](release.md): If you want to release a dataset collected on Crowdaq, what're the things to publish so that future requesters can easily reproduce or extend your dataset.
5. [Syntax](https://github.com/Crowdaq/crowdaq/tree/main/packages/schema/crowdaq/json_schema): Our source JSON schemas showing the syntax of instructions, tutorials, exams, and task sets.
